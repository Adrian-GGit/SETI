\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{graphicx}

\begin{document}

\tableofcontents
\thispagestyle{empty}
\newpage

\section{Einleitung}
Hier eine kurze Einleitung in welchem Rahmen diese Arbeit entstanden ist und schonmal ganz kurz auf SETI eingehen.

\section{SETI Breakthrough Listen}
Die kaggle Challenge \emph{SETI Breakthrough Listen - E.T. Signal Search} war ein öffentlicher Machine Learning Wettbewerb des \emph{Berkeley SETI Research Centers} im Zeitraum vom 10. Mai 2021 bis 18. August 2021. Die zugrunde liegenden Daten sind noch verfügbar, sodass Interessierte sich nach wie vor mit diesem Problem beschäftigen können. Im folgenden werden wir die Challenge stets abgekürzt als \emph{SETI} bezeichnen. 

Die Herausforderung bei \emph{SETI} besteht darin, Spektrogramme, also eine bildliche Darstellung eines Frequenzbereichs in einem bestimmten Zeitraum, die basierend auf Rohdaten des \emph{Green Bank Telescopes} generiert worden sind, auf das Vorkommen von künstlich hinzugefügten extraterrestrischen Signalen zu untersuchen. Hierbei ist es wichtig, diese Signale von irdisches Signalen, wie etwa einem Radiosignal, zu unterscheiden. Um diese Unterscheidung vornehmen zu können, sind jeweils sechs Spektrogramme zusammengefasst, wobei die Spektrogramme eins, drei und fünf jeweils Aufnahmen des zu untersuchenden Ziels \glqq A\grqq{} sind und die übrigen jeweils auf Aufnahmen eines anderen Himmelskörpers \glqq B\grqq{}, \glqq C\grqq{} und \glqq D\grqq{}. Eine solche Gruppe von Spektrogrammen (ABACAD) wird bei \emph{SETI} als \emph{Kadenz-Ausschnitt}, im folgenden nur noch \glqq Kadenz\grqq{}, bezeichnet. Jedes Spektrogramm zeigt den Frequenzbereich für einen Zeitraum von fünf Minuten, eine Kadenz stellt folglich einen Beobachtungszeitraum von 30 Minuten dar.

Abbildung \ref{fig:kadenz_pos_1} zeigt ein Beispiel für eine Kadenz mit einem extraterrestrischen Signal. Die drei Spektrogramme in der oberen Zeile sind \emph{on target}, auf diesen ist im Frequenzbereich, welcher durch die x-Achse repräsentiert ist, zwischen 150 und 200 ein Signal zu erkennen, welches auf den Spektrogrammen in der unteren Zeile, welche \emph{off target} sind, nicht zu sehen ist. Die grüne senkrechte Linie, die auf allen sechs Spektogrammen zu sehen ist, ist hingegen ein irdisches Signal. Offensichtlich muss ein Signal nicht auf jedem der drei \emph{on target} Spektrogrammen zu sehen sein, da ein Signal nicht zwingend über den gesamten zeitlichen Betrachtungsraum aktiv sein muss.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\textwidth]{"img/kadenz_pos_1.png"}
\caption{Beispiel für ein extraterrestrisches Signal}
\label{fig:kadenz_pos_1}
\end{figure}

Die Trainingsdaten für \emph{SETI} enthalten 60.000 Kadenzen (\emph{Heuhaufen}) von denen 6.000 \emph{Nadeln} sind, also Kadenzen, die ein künstlich eingefügtes extraterrestrisches Signal enthalten. Einige dieser Signale sind bei entsprechender Visualisierung sofort mit bloßem Auge zu erkennen, andere sind in, durch irdische Signale verursachten, Rauschen versteckt.


\section{Implementierung}
Im folgenden beschäftigen wir uns nun mit der Problemlösung für \emph{SETI}. Wir schauen uns erste Ansätze mit reiner Computer Vision an, die uns helfen sollen auch versteckte Signale extrahieren zu können, um sie mit bloßem Auge erkennen zu können. Wir haben uns für diesen Einstieg entschieden, um ein Gefühl für die visuelle Form der gesuchten Signale und für den Datensatz allgemein zu erhalten. Im darauf folgenden Kapitel werden wir uns mit \emph{Convulutional Neural Networks}, kurz \emph{CNNs}, beschäftigen.

\subsection{Erste Ansätze mit Computer Vision}
Wenn man über Bilderkennung spricht denkt man meistens direkt an Machine Learning und Neurale Netzwerke. Allerdings gibt es in der Bilderkennung Problematiken, die sich mithilfe der klassischen Computer Vision deutlich besser lösen lassen. Im Folgenden wird auf die grundlegenden Unterschiede zwischen der klassischen Computervision und Machine Learning eingegangen, einige Grundlagen beschrieben, Lösungen der Computervision angewandt auf die Problematik der Projektarbeit und ob die klassische Computervision für diese Projektarbeit geeignet ist.

\subsubsection{Unterschiede Computervision - Machine Learning}
Computervision ist nicht gleichzusetzen mit Machine Learning. Die klassische Computervision arbeitet mit rein mathematischen Ansätzen und Algorithmen, während das klassische Machine Learning eher darauf bedacht ist mithilfe von Trainingsdaten Modelle zu trainieren und dadurch Parameteranpassungen durchzuführen. Computervision ist sehr vielfältig einsetzbar. Mithilfe von Algorithmen können beispielsweise alle möglichen geometrische Primitive detektiert werden oder auch mithilfe von sogenannten Filtern das Bild entsprechend der Filter verarbeitet werden, weswegen anfangs dieser Projektarbeit mit einigen Ansätzen der Computervision experimentiert wurde. Diese Filter werden außerdem in den nachfolgenden Kapiteln noch sehr interessant für die Convolutional Neural Networks.

\subsubsection{Grundlagen}
Bilder sind im Prinzip nichts weiter als eine große Ansammlung von Zahlen, welche die entsprechenden Farben repräsentieren. In der klassischen Computervision arbeitet man häufig mit sogenannten Grauwertbildern. Diese sind besonders einfach zu handhaben, da diese nur einen Farbkanal besitzen im Gegensatz zu RGB Farbbildern, welche 3 Farbkanäle besitzen.

\subsubsection{Lokale Operatoren: Filter}
Lokale Operatoren sind sogenannte Punktoperationen, also Operationen die auf jedem Pixel des Bildes angewandt werden. Bei Lokalen Operatoren ist die Besonderheit, dass benachbarte Pixel auch mit in die Berechnung der Operation einfließen und somit auch den Farbwert des betrachteten Pixels beeinflussen. Somit können zum Beispiel Grauwertübergänge, also Übergänge von dunkel nach hell, detektiert werden. Um diese besonderen Punktoperationen besser zu verstehen, muss man das Bild als eine drei-dimensionale Funktion betrachten: die x Achse steht für die Breite des Bildes, y ist die Höhe des Bildes und z die Farbe des entsprechenden Pixels an der Stelle (x,y), so erhält man eine Grauwertfunktion g(x,y)=z. An den Stellen wo es einen schnellen Wechsel von dunkel zu hell gibt, hat die Ableitung der Grauwertfunktion einen Extrempunkt:
\newline

g'(x,y)=[$\frac{\partial g(x,y)}{\partial x}$, $\frac{\partial g(x,y)}{\partial y}$]$^T$
\newline
Für diskrete Bilder muss die Ableitung allerdings angenähert werden, da ein Pixel nur einen einzigen Wert beinhaltet:
\newline

$\frac{\partial g(x,y)}{\partial x}$ = $\frac{g(x+\Delta x,y)-g(x-\Delta x,y)}{\Delta x}$
\newline

$\frac{\partial g(x,y)}{\partial y}$ = $\frac{g(x,y+\Delta y)-g(x,y-\Delta y)}{\Delta y}$
\newline
Setzt man nun $\Delta x$ beziehungsweise $\Delta y$ gleich 1 erhält man folgende Approximationen:
\newline

$\frac{\partial g(x,y)}{\partial x}$ = $\frac{g(x+1,y)-g(x-1,y)}{1}$
\newline

$\frac{\partial g(x,y)}{\partial y}$ = $\frac{g(x,y+1)-g(x,y-1)}{1}$
\newline
Für jedes Pixel (x,y) auf welches diese Approximation angewandt wird, wird also folgendes berechnet:
\newline

$1\cdot g(x+1,y)+0\cdot g(x,y)-1\cdot g(x-1,y)$
\newline
Wir erhalten also folgendes Operatorfenster:
\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{img/operatorfenster.png}
\end{figure}
\newline
Mit dem Operatorfenster kann auf jedem einzelnen Pixel eine sogenannte gewichtete Addition ausgeführt werden. Dafür legt man das Operatorfenster auf eine entsprechende Stelle des Bildes, multipliziert die einzelnen Komponenten des Fensters mit den darunterliegenden Werten der Pixel und summiert diese Produkte. Anstatt eines 3x1 Operatorfensters können auch 3x3 Operatorfenster genutzt werden, wodurch auch Pixel die über und unter dem zu betrachtenden Pixel mit in die Berechnung einfließen. Das Ergebnis der Anwendung des oben genannten Operatorfensters ist ein neues Bild welches insbesondere vertikale Grauwertübergänge hervorhebt und horizontale Grauwertübergänge entfernt. Der Grund dafür ist die Approximation der ersten Ableitung in x-Richtung, wodurch die Grauwertfunktion des Bildes abgeleitet wird. Dadurch kann man das resultierende Bilde sozusagen als Ableitung des Originalgrauwertbildes betrachten: Stellen die hervorgehoben werden, sind starke Grauwertübergänge und horizontale Linien werden entfernt, da auf einer gleichfarbigen horizontalen Linie keine Grauwertübergänge existieren. Dasselbe gilt umgekehrt für die Approximation in y-Richtung - dafür muss das Operatorfenster einfach nur um 90 Grad gedreht werden. Mit der Approximation in y-Richtung werden vertikale Linien entfernt.
\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{img/original-vs-cv.png}
\end{figure}
\newline
Die Idee hinter der Verwendung der Filter ist die, dass die Signale alle eine vertikale Richtung aufweisen. Mithilfe der Filter für die Eleminierung der horizontalen Linien kann also Rauschen recht einfach rausgefiltert werden.
TODO: Referenz auf Laubenheimer Folien

\subsubsection{Weitere Experimente}
Das Rauschen in den Bildern ist häufig sehr ausgeprägt, weshalb Algorithmen zur Rauschminderung zum Einsatz kamen. Die Idee hinter dem ersten Algorithmus war ähnliche Pixel auf dieselbe Farbe abzubilden. Anfangs wird die Anzahl der unterschiedlichen Farben gezählt und anschließend die mittlere Differenz der Farbwerte berechnet. Anhand der mittleren Differenz und ein als Parameter festgelegter Wert, der bestimmt wie stark die mittlere Differenz den Algorithmus beeinflusst, kann ein neuer Wert berechnet werden, der bestimmt wie unterschiedlich ähnliche Pixel zueinander sein dürfen, sodass sie auf dieselbe Farbe abgebildet werden. 
\newline
Nachdem nun dieser Algorithmus zur Abbildung ähnlicher Pixel auf dieselbe Farbe angewandt wurde, kann es sein dass die unterschiedlichen Farben teilweise einen geringeren, aber auch einen höheren Abstand zum jeweils nächsten Farbwert haben. Die Lösung dafür ist, dass genau diese Differenz auf dieselbe mittlere Differenz gesetzt wird. Dadurch erreicht man, dass die Farben, die bislang noch sehr nah beieinander lagen nach Anwendung dieses Algorithmus besser zu unterscheiden sind und Farben, die bislang eher weiter weg voneinander lagen, immer noch gut voneinander unterscheidbar sind. Anfangs wird wieder die Anzahl der unterschiedlichen Farben gezählt und auch die mittlere Differenz zwischen den einzelnen Farben berechnet. Die Farben sollen nach Anwendung des Algorithmus genau diese mittlere Differenz zueinander haben, dies geschieht indem die einzelnen Farben nacheinander auf jeweils ein Vielfaches der mittleren Farbdifferenz gesetzt werden.
\newline
Führt man vor diesen beiden Algorithmen noch den lokalen Operator zur Entfernung der horizontalen Linien aus erhält man beispielsweise folgende Transformation:
\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{img/original-vs-rauschminderung.png}
\end{figure}
\newline
Auf dem Originalbild ist mit dem bloßen Auge kein Signal zu finden, betrachtet man allerdings das transformierte Bild ist ein schwaches Signal durchaus zu erkennen.

\subsubsection{Zwischenfazit: Computervision alleine bringt keinen Erfolg}
Computervision ist zwar sehr vielfältig einsetzbar, allerdings für die Problematik dieser Projektarbeit nur sehr schwer anwendbar. Das liegt insbesondere daran, dass die Signale teilweise im Hintergrundrauschen sehr gut versteckt sind und teilweise nicht mal mit dem Auge zu erkennen sind. Mit reiner Algorithmik und klassischen Computervisionansätzen, wie Liniendetektion sind die Signale fast nicht zu entdecken, da einerseits die Signale nur sehr schwer vom Hintergrund separierbar sind und andererseits viele der Signale keine geraden Linien sind, sondern alle möglichen undefinierbaren Formen annehmen können. Allerdings könnten die oben beschriebenen Vorgehensweisen mit den Filtern und Algorithmen zur Rauschunterdrückung genutzt werden um ein Modelltraining mit einem Convolutional Neural Network zu unterstützen, aber dazu mehr in den nachfolgenden Kapiteln.

\subsubsection{Computervision verwenden als Vorverarbeitung für das Modelltraining}
TODO: Bilder vorverarbeiten mit Sobelfilter (vertikale Linien entfernen) und danach Modell trainieren (10 Epoch falls das nicht zu lange ist) mit vorverarbeiteten Bildern

\section{Deep Learning}
Natürlich wollen wir nicht alle Kadenzen einzeln manuell betrachten und entscheiden, ob sie eine Nadel enthalten oder nicht. Vielmehr wollen wir ein Machine Learning Model trainieren, das uns diese Arbeit abnimmt und Nadeln findet, die wir gar nicht entdecken würden. Hierzu wollen wir ein \emph{Convolutional Neural Network (CNN)} trainieren, das die Kadenzen in genau zwei Klassen einteilt: enthält eine Nadel oder enthält keine Nadel.

\subsection{Convolutional Neural Networks}
Hier können wir uns nochmal überlegen oder mit Herrn Baier besprechen, wie doll wir bei den Erklärungen zu den einzelnen Punkten von CNNs ins Detail gehen sollen. Vll können wir auch vieles als bekannt voraussetzen und uns mehr auf unsere Implementierung konzentrieren. Dies wäre insgesamt vll nochmal abzuklären auch für die CV Section.

\begin{itemize}
	\item Gewichte
	\item Loss Function
	\item Gradient
	\item Optimizer
	\item Traning und Validierung
	\item Splitten der Trainingsdaten
	\item Dataloaders
	\item Metriken (Accuracy, Precision, Recall, F1 Score, Roc Auc Score)
\end{itemize}

\subsection{Transfer Learning}

\subsubsection{efficientnet}

\subsection{Imbalance}

\subsection{Scheduler}

\subsection{Folds}

\section{Tech Stack}
Zusammenfassung der verwendeten Tools und Bibliotheken, die teilweise auch vorher im Text schon genannt wurden.

\section{Fazit}


\end{document}