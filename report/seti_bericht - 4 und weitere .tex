\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{graphicx}
\usepackage{listings}

\begin{document}

\section{Imbalance}
Unter Imbalanced Datasets, also unausgeglichenen Datensätzen, versteht man die Problematik, dass mindestens ein Label in einem Datensatz häufiger vertreten ist als andere Labels. Das Problem bei Imbalanced Datasets ist, dass das Modell mehr darauf trainiert wird die häufiger vorkommenden Labels zu erkennen, da entsprechende Labels öfter vom Modell gesehen werden und entsprechend stärker das Modell beeinflussen als andere Labels. Für diese Problematik gibt es einige Lösungsansätze:
\newline
Oversampling ist eine hervorragende Lösung um aus einem imbalanced Dataset ein balanced Dataset zu erschaffen. Dabei werden die weniger vorkommenden Labels entsprechend zufällig vervielfacht um ein ausgewogenes Verhältnis zwischen allen Labels zu schaffen. Natürlich gibt es noch einige weitere Strategien zum Beispiel das genaue Gegenteil von Oversampling: das Undersampling. Es werden zufällig Daten von den häufig vorkommenden Labels aus dem Datensatz entfernt. Eine andere Strategie setzt bei der Loss-Berechnung an: Die Gewichtung der entsprechenden Labels kann in der Berechnung der Loss-Funktion erhöht werden, wodurch der Loss entsprechend hoch ist bei falscher Vorhersage und somit die Parameter auch entsprechend stark angepasst werden müssen. Der Rest dieses Kapitels fokussiert sich insbesondere auf die Strategie des Oversamplings.
\newline
Bei dem SETI Datensatz handelt es sich um ein Imbalanced Dataset: die Labels mit 0, also "kein Signal", sind zu 90\% vertreten während Labels mit 1 nur zu 10\% im Dataset vorzufinden sind. In Pytorch lässt sich Oversampling mithilfe des "WeightedRandomSampler" recht einfach implementieren:
\begin{lstlisting}
labels = np.array(dataset_train.labels)
class_counts = np.array(
	[len(np.where(labels == t)[0]) 
	for t in np.unique(labels)])
num_samples = sum(class_counts)
class_weights = [
	num_samples/class_counts[i] 
	for i in range(len(class_counts))]
weights = [
	class_weights[labels[i]] 
	for i in range(int(num_samples))]
sampler = WeightedRandomSampler(
	torch.DoubleTensor(weights), int(num_samples))
train_dataloader = DataLoader(
	dataset_train, 
	batch_size=conf_dict["batch_size"], 
	sampler=sampler)
\end{lstlisting}
Anfangs wird gezählt wie viele Labels jeweils im Datensatz vorhanden sind und die Verhältnisse von Gesamtanzahl der Labels zu den einzelnen Vorkommen der Labels wird für jedes einzelne Label berechnet. Die berechneten Zahlen sind die Gewichte der einzelnen Klassen. Diese werden für den "WeightedRandomSampler" benötigt, welcher im "DataLoader" entsprechend dafür sorgt, dass durchschnittlich alle verschiedenen Labels gleich wahrscheinlich für das Training genutzt werden. Der entsprechende Dataloader kann nun für das Training verwendet werden. Im Folgenden wird der reguläre Trainingsansatz mit dem des Oversamplings verglichen und auch mit einer hybriden Variante. Die hybride Variante verbindet das normale Training mit einem unbalanced Dataset mit dem Oversampling: alle Epochen bis auf die letzte werden mit einem balanced Dataset trainiert, in der letzten Epoche trainiert das Modell mit dem original Datensatz.
\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{metrics-acc-recall-prec.png}
\end{figure}
Das Training mit dem original Datensatz hat die höchste Accuracy, allerdings auch den geringsten Recall. Das liegt daran, dass das Modell fast nur darauf trainiert wird 0er Labels zu erkennen und hat daher zwar eine hohe Accuracy, dafür aber einen geringen Recall, da es die meisten 1er Labels nicht als solche interpretiert. Die Precision dagegen ist aber hoch, was bedeutet dass viele der als Signal vorhergesagten Labels auch tatsächlich Signale sind. Da der SETI Datensatz einige Kadenzen aufweist wo die Signale auch mit bloßem Auge gut zu erkennen sind, liegt die Annahme nahe, dass das Modell vor allem die leicht zu findenden Signale erkennt. Ähnlich verhält es sich mit der hybriden Variante, allerdings ist der Recall etwas höher als bei der original Variante. Das liegt daran, dass überwiegend mit einem ausgeglichenem Datensatz trainiert wurde, wodurch das Modell mehr 1er Labels erkennt. Die gewichtete Variante hängt bei der Accuracy und Precision hinterher, dafür ist der Recall bei dieser Variante fast am höchsten, da ausschließlich mit einem augeglichenem Datensatz trainiert wurde. Außerdem ist auffällig dass die Metriken für die Validierungsdaten bei dem ausgeglichenen und hybriden Datensatz von den Metriken der Trainingsdaten abweichen, das hat den Grund dass die Validierungsdaten nicht an das Training angepasst wurden. Validierungsdaten sollten nie verändert werden und insbesondere nicht für das Training eingesetzt werden. Des Weiteren ist auffällig, dass das Training mit den ausgeglichenen Daten zwar eine recht hohe Accuracy und Recall liefert, allerdings leidet die Precision darunter je mehr das Modell gelernt hat. Das hängt unter anderem damit zusammen, dass der Recall immer weiter steigt und das Modell somit immer mehr 1er Labels als Signal interpretiert wobei viele davon gar keine Signale darstellen. Möglicherweise ist das Modell mit welchem trainiert wurde - efficientnet-b1 - nicht komplex genug um diese Aufgabe zu bewältigen. Das lässt sich auch recht gut aus den nächsten Diagrammen herauslesen:
\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{metrics-avgloss-rocauc.png}
\end{figure}
Am interessantesten ist der Graph mit dem durchschnittlichen Loss. Sowohl bei der hybriden Variante als auch bei der original Variante ist der Loss recht niedrig gegen Ende der Epochen, während der Loss des Modells mit dem ausgeglichenen Datensatz auch am Ende noch recht hoch ist. Das hängt wie oben bereits erwähnt möglicherweise damit zusammen, dass das Modell nicht komplex genug ist. Da viele der Vorhersagen als Signal interpretiert werden, allerdings viele davon auch falsch sind, ist der Loss entsprechend hoch. Der Roc/Auc Score ist bei allen Varianten gegen Ende fast gleich - der Score der gewichteten Variante ist fast am höchsten, da das Verhältnis von Recall und Precision besser ist. Da wie oben bereits angesprochen das Modell efficientnet-b1 möglicherweise nicht komplex genug für die Problematik ist, wurde mit einem komplexeren Modell efficientnet-b5 trainiert und außerdem mit einem ausgeglichenen Datensatz. Die Metriken für das Modell sind in den oben abgebildeten Diagrammen bereits enthalten. Die Verläufe der Metriken ähneln sehr den Verläufen der anderen ausgeglichenen Variante und der hybriden Variante. Allerdings sind die Metriken überwiegend besser, was die Hypothese bestätigt, dass das efficientnet-b5 besser für die Aufgabe geeignet ist als das efficientnet-b1.
\newline
Abschließend kann man sagen, dass sowohl die Variante mit den original Daten als auch die Variante mit den gewichteten Daten beide Vor- und Nachteile haben. Daher ist die hybride Variante ein guter Kompromiss um die Vorteile beider Varianten auszunutzen. Allerdings muss man beim Oversampling aufgepassen, dass das Modell nicht übertrainiert wird auf die mehrfach genutzten Daten. Dabei kann uns Dataaugmentation weiterhelfen.

TODO: Referenz https://arxiv.org/abs/1710.05381

\section{Dataugmentation}
Gerade für die Strategie des Oversamplings ist es wichtig aufzupassen, dass das Modell nicht overfittet, also zu sehr auf spezielle Daten trainiert ist und somit auf allgemeinen Daten schlecht abschneidet. Da man beim Oversampling Daten mehrmals für dasselbe Training nutzt ist es sinnvoll die Daten etwas abzuändern, mithilfe von Dataaugmentation. Dabei geht es darum die Daten so zu transformieren, dass sie für das Training immer noch brauchbar sind, aber nicht mehr dem Original entsprechen. Dabei kann das Bild beispielsweise einfach horizontal oder vertikal gespiegelt werden. Es gibt noch viele weitere Möglichkeiten wie Rotieren, das Bild vergrößern und noch einige weitere Varianten. Diese Transformationen geschehen dabei zufällig, also mal wird beispielsweise nur horizontal gespiegelt, mal bleibt das Original erhalten, mal werden alle vorgegebenen Transformationen angewandt. Dadurch erhält man ein komplett neues Bild mit welchem das Modell trainieren kann ohne zu overfitten. Aber Dataaugmentation kann nicht nur sinnvoll sein, wenn man Oversampling angewandt hat, es kann auch sinnvoll sein wenn man ganz normale Trainingsdaten hat und das Modell möglichst breit gefächert auf den Daten lernen lassen möchte. Durch die Transformationen erhält man sozusagen alle möglichen weiteren Szenarien die das Modell erlernen kann.

TODO: Referenz Buch "Deep Learning mit Python und Keras von Francois Chollet"

\section{Scheduler}
Trainiert man ein Modell, spielt die Lernrate eine große Rolle, gerade wenn es darum geht die optimalen Parameter zu erhalten. Ist die Lernrate zu klein, dauert es lange bis die optimalen Werte erreicht werden, während bei einer zu hohen Lernrate dieser optimale Punkt übersprungen werden könnte:
\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{lernrates-vs.png}
\end{figure}
Ein sogenannter Scheduler kann die Suche nach einer optimalen Lernrate Abhilfe schaffen. Die Grundidee des Schedulers ist die Lernrate über die Epochen hinweg zu verringern. Je höher die Epoche also je mehr man bereits trainiert hat, desto näher ist man am optimalen Punkt und die Lernrate muss immer weiter verringert werden um diesen Punkt nicht zu verpassen. Dadurch erhält man sowohl Vorteile einer hohen Lernrate als auch die Vorteile einer geringen Lernrate: Anfangs ist die Lernrate noch hoch wodurch man sich dem Optimum schnell nähert, aber je näher man dem Optimum näher kommt, desto geringer wird die Lernrate und man veroasst das Optimum nicht.

TODO: Referenz https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/lr_scheduling/

\section{K Fold Cross Validation}
Wenn man wissen möchte wie gut ein Modell trainiert ist, kann man wie bereits weiter oben erwähnt bestimmte Metriken berechnen lassen, die das Modell bewerten. Dabei kann es rein statistisch gesehen vorkommen, dass das Modell mal besonders schlecht repräsentative Trainingsdaten erhält und somit auf realen Daten schlecht abschneidet. Um robuste Metriken zu berechnen hilft die K Fold Cross Validation. Dabei wird der gesamte Datensatz in k viele Teile unterteilt, wobei es entsprechend k viele Folds gibt. In jedem Fold wird das Modell neu trainiert, allerdings immer wieder mit anderen Trainings- und Validierungsdaten. Die Validierungsdaten beim ersten Fold sind beispielsweise der erste der k Teile, beim zweiten Fold der zweite und so weiter:
\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{folds.png}
\end{figure}
Die Metriken können nach Abschluss aller Folds gemittelt werden, wodurch man robuste Werte erhält. Außerdem kann ermittelt werden in welchem Fold das Modell am besten abgeschnitten hat.

TODO: Referenz https://medium.com/dataseries/k-fold-cross-validation-with-pytorch-and-sklearn-d094aa00105f

\end{document}

